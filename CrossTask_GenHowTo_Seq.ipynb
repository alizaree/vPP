{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crosstask\n",
      "./models/GenHowTo/weights/GenHowTo-STATES-96h-v1\n"
     ]
    }
   ],
   "source": [
    "CUDA_VISIBLE_DEVICES=0\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.model_name = 'crosstask'\n",
    "        self.weights_path = \"./models/GenHowTo/weights/GenHowTo-STATES-96h-v1\"\n",
    "        self.device = \"cuda\"\n",
    "        self.num_inference_steps = 50\n",
    "        self.num_steps_to_skip = None\n",
    "        self.guidance_scale = 9.0\n",
    "        self.max_traj_len = 3\n",
    "        self.dataset = 'crosstask'\n",
    "        self.num_action = 133\n",
    "        self.num_tasks = 18\n",
    "        self.epochs = 500\n",
    "        self.batch_size = 1\n",
    "        self.dropout = 0.2\n",
    "        self.optimizer = 'adam'\n",
    "        self.lr = 0.004\n",
    "        self.step_size = 40\n",
    "        self.lr_decay = 0.65\n",
    "        self.weight_decay = 0.0001\n",
    "        self.M = 2\n",
    "        self.aug_range = 0\n",
    "        self.no_state_task = False\n",
    "        self.root_dir = 'dataset/crosstask/crosstask_release'\n",
    "        self.train_json = 'dataset/crosstask/cross_task_data_False.json'\n",
    "        self.valid_json = 'dataset/crosstask/cross_task_data_True.json'\n",
    "        self.features_dir = '/home/yulei/data/crosstask/processed_data/'\n",
    "        self.vid_dir = '/home/yulei/data/crosstask/crosstask_videos/videos/'\n",
    "        self.img_dir = '/dvmm-filer3a/users/ali/Data/CrossTask/crosstask_frame_states'\n",
    "        self.return_frames = True\n",
    "        self.save_image_states = False\n",
    "        self.eval_mode = False\n",
    "        self.saved_path = 'checkpoints'\n",
    "        self.last_epoch = -1\n",
    "        self.split = 'base'\n",
    "        self.seed = 3407\n",
    "        self.uncertain = False\n",
    "        self.num_sample = 1500\n",
    "\n",
    "# Create an instance of Args\n",
    "args = Args()\n",
    "\n",
    "# Use the variables in your code\n",
    "# For example:\n",
    "print(args.model_name)\n",
    "print(args.weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "from utils import *\n",
    "from metrics import *\n",
    "from torch.utils.data import DataLoader\n",
    "from models.procedure_model import ProcedureModel\n",
    "from models.utils import AverageMeter\n",
    "import wandb\n",
    "from tools.parser import create_parser\n",
    "from PIL import Image\n",
    "\n",
    "from models.GenHowTo.genhowto_utils import load_genhowto_model, DDIMSkipScheduler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "if args.dataset == 'crosstask':\n",
    "    if args.split == 'base':\n",
    "        from dataset.my_crosstask_dataloader import CrossTaskDataset as ProcedureDataset\n",
    "    elif args.split == 'pdpp':\n",
    "        # use PDPP data split and data sample\n",
    "        from dataset.crosstask_dataloader_pdpp import CrossTaskDataset as ProcedureDataset\n",
    "    elif args.split == 'p3iv':\n",
    "        # use P3IV data split and data sample\n",
    "        assert args.max_traj_len == 3, \"Only the datasplit for max_traj_len = 3 is available.\"\n",
    "        from dataset.crosstask_dataloader_p3iv import CrossTaskDataset as ProcedureDataset\n",
    "\n",
    "elif args.dataset == 'coin':\n",
    "    from dataset.coin_dataloader import CoinDataset as ProcedureDataset\n",
    "\n",
    "elif args.dataset == 'niv':\n",
    "    from dataset.niv_dataloader import NivDataset as ProcedureDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-19 16:50:51,901][1396166359.py][line:10][INFO] logs/2024-04-19-16-50-51_crosstask_len3/log.txt\n",
      "[2024-04-19 16:50:51,902][1396166359.py][line:11][INFO] <__main__.Args object at 0x7fd1a8b01f60>\n",
      "[2024-04-19 16:50:51,904][1396166359.py][line:18][INFO] Loading prompt features...\n",
      "[2024-04-19 16:51:04,340][1396166359.py][line:33][INFO] Loading training data...\n",
      "Processing videos: 100%|██████████| 2200/2200 [00:46<00:00, 47.35it/s] \n",
      "[2024-04-19 16:51:50,818][1396166359.py][line:40][INFO] Loading valid data...\n",
      "Processing videos: 100%|██████████| 340/340 [00:07<00:00, 44.79it/s]\n",
      "[2024-04-19 16:51:58,414][1396166359.py][line:83][INFO] Training set volumn: 11884 Testing set volumn: 1958\n",
      "[2024-04-19 16:52:11,293][1396166359.py][line:89][INFO] the model is loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger_path = \"logs/{}_{}_len{}\".format(\n",
    "                time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime()), \n",
    "                args.model_name, \n",
    "                args.max_traj_len)\n",
    "if args.last_epoch > -1:\n",
    "    logger_path += \"_last{}\".format(args.last_epoch)\n",
    "os.makedirs(logger_path)\n",
    "log_file_path = os.path.join(logger_path, \"log.txt\")\n",
    "logger = get_logger(log_file_path)\n",
    "logger.info(\"{}\".format(log_file_path))\n",
    "logger.info(\"{}\".format(args))\n",
    "\n",
    "validate_interval = 1\n",
    "setup_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if args.dataset == 'crosstask':\n",
    "    logger.info(\"Loading prompt features...\")\n",
    "    file_path = './data/descriptors_crosstask.json'\n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r') as f:\n",
    "        state_prompts = json.load(f)\n",
    "    #state_prompt_features = np.load(f'./data/state_description_features/crosstask_state_prompt_features.npy')\n",
    "\n",
    "    ## parse raw data\n",
    "    task_info_path = os.path.join(args.root_dir, \"tasks_primary.txt\")\n",
    "    task_info = parse_task_info(task_info_path)\n",
    "    with open(\"data/crosstask_idices.json\", \"r\") as f:\n",
    "        idices_mapping = json.load(f)\n",
    "    anot_dir = os.path.join(args.root_dir, \"annotations\")\n",
    "    anot_info = parse_annotation(anot_dir, task_info, idices_mapping)\n",
    "\n",
    "    logger.info(\"Loading training data...\")\n",
    "    train_dataset = ProcedureDataset(anot_info, args.img_dir, state_prompts, \n",
    "                                    args.train_json, args.max_traj_len, aug_range=args.aug_range, \n",
    "                                    mode = \"train\", M=args.M,\n",
    "                                    vid_dir=args.vid_dir,\n",
    "                                    save_image_states=args.save_image_states)\n",
    "    \n",
    "    logger.info(\"Loading valid data...\")\n",
    "    valid_dataset = ProcedureDataset(anot_info, args.img_dir, state_prompts, \n",
    "                                    args.valid_json, args.max_traj_len, aug_range=args.aug_range, \n",
    "                                    mode = \"valid\", M=args.M,\n",
    "                                    vid_dir=args.vid_dir,\n",
    "                                    save_image_states=args.save_image_states)\n",
    "    transition_matrix = train_dataset.transition_matrix\n",
    "    \n",
    "elif args.dataset == \"coin\":\n",
    "    raise NotImplementedError\n",
    "    logger.info(\"Loading prompt features...\")\n",
    "    state_prompt_features = np.load(f'./data/state_description_features/coin_state_prompt_features.npy')\n",
    "\n",
    "    logger.info(\"Loading training data...\")\n",
    "    train_dataset = ProcedureDataset(args.features_dir, state_prompt_features, \n",
    "                                    args.train_json, args.max_traj_len, aug_range=args.aug_range, \n",
    "                                    mode = \"train\", M=args.M)\n",
    "    \n",
    "    logger.info(\"Loading valid data...\")\n",
    "    valid_dataset = ProcedureDataset(args.features_dir, state_prompt_features, \n",
    "                                    args.valid_json, args.max_traj_len, aug_range=args.aug_range, \n",
    "                                    mode = \"valid\", M=args.M)\n",
    "    transition_matrix = train_dataset.transition_matrix\n",
    "\n",
    "elif args.dataset == \"niv\":\n",
    "    raise NotImplementedError\n",
    "    logger.info(\"Loading prompt features...\")\n",
    "    state_prompt_features = np.load(f'./data/state_description_features/niv_state_prompt_features.npy')\n",
    "\n",
    "    logger.info(\"Loading training data...\")\n",
    "    train_dataset = ProcedureDataset(args.features_dir, state_prompt_features, \n",
    "                                    args.train_json, args.max_traj_len, num_action = 48,\n",
    "                                    aug_range=args.aug_range, mode = \"train\", M=args.M)\n",
    "    \n",
    "    logger.info(\"Loading valid data...\")\n",
    "    valid_dataset = ProcedureDataset(args.features_dir, state_prompt_features,\n",
    "                                    args.valid_json, args.max_traj_len, num_action = 48,\n",
    "                                    aug_range=args.aug_range, mode = \"valid\", M=args.M)\n",
    "    transition_matrix = train_dataset.transition_matrix\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "logger.info(\"Training set volumn: {} Testing set volumn: {}\".format(len(train_dataset), len(valid_dataset)))\n",
    "# Initialize wandb\n",
    "#wandb_config=vars(args)\n",
    "#wandb.init(project='vPP', config=wandb_config)\n",
    "\n",
    "pipe = load_genhowto_model(args.weights_path, device=device)\n",
    "logger.info(\"the model is loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:11<00:00,  4.41it/s]\n",
      "100%|██████████| 50/50 [00:11<00:00,  4.38it/s]\n",
      "100%|██████████| 50/50 [00:11<00:00,  4.37it/s]\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    #  data[0] : batch*n_act*2(start, end frames)*w*h*3(RGB)\n",
    "    # data[1]: list [ac_1,ac_2,ac_3] ac_i[key:description, before, after]=batch*[str] \n",
    "    # data[3]: data[3].shape torch.Size([256]) of task ids\n",
    "    pipe.scheduler.set_timesteps(args.num_inference_steps)\n",
    "    \n",
    "    #set the scheduler of GenHowTo (on per instance bases)\n",
    "    if args.num_steps_to_skip is not None:  # possibly do not start from complete noise\n",
    "        pipe.scheduler = DDIMSkipScheduler.from_config(pipe.scheduler.config)\n",
    "        pipe.scheduler.set_num_steps_to_skip(args.num_steps_to_skip, args.num_inference_steps)\n",
    "        print(f\"Skipping first {args.num_steps_to_skip} DDIM steps, i.e., running DDIM from timestep \"\n",
    "            f\"{pipe.scheduler.timesteps[0]} to {pipe.scheduler.timesteps[-1]}.\")\n",
    "        \n",
    "    input=data[0][:,0,0,...].clone() \n",
    "    final=data[0][:,-1,0,...].clone() \n",
    "    img_final= [Image.fromarray(( idd.numpy()).astype(np.uint8)) for idd in final]\n",
    "    img_input = [Image.fromarray(( idd.numpy()).astype(np.uint8)) for idd in input]\n",
    "    all_outs=[img_input]\n",
    "    all_prompts=[]\n",
    "    all_actions=[]\n",
    "    for action_id in range(args.max_traj_len):\n",
    "        for state_mode in ['after']:\n",
    "            prompt=[data[1][action_id][state_mode][idd][0] for idd in range(args.batch_size)]\n",
    "            action=[data[1][action_id]['description'][idd] for idd in range(args.batch_size)]\n",
    "            latents = torch.randn((args.batch_size, 4, 64, 64))\n",
    "            if args.num_inference_steps is not None:\n",
    "                z = pipe.control_image_processor.preprocess(img_input)\n",
    "                z = z * pipe.vae.config.scaling_factor\n",
    "                t = pipe.scheduler.timesteps[0]\n",
    "                alpha_bar = pipe.scheduler.alphas_cumprod[t].item()\n",
    "                latents = math.sqrt(alpha_bar) * z + math.sqrt(1. - alpha_bar) * latents.to(z.device)\n",
    "            \n",
    "            \n",
    "            #prompt_embeds=pipe.text_encoder(torch.tensor(pipe.tokenizer(prompt)['input_ids']).to(device)).last_hidden_state\n",
    "                \n",
    "            output = pipe(\n",
    "            image=img_input,prompt=prompt,\n",
    "            guidance_scale=args.guidance_scale,\n",
    "            num_inference_steps=args.num_inference_steps,\n",
    "            latents=latents,\n",
    "            num_images_per_prompt=1,\n",
    "            return_dict=True,\n",
    "            ).images\n",
    "            all_outs.append(output)\n",
    "            all_prompts.append(prompt)\n",
    "            all_actions.append(action)\n",
    "            img_input=[idd for idd in output]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  Mix ingredients together for the banana ice cream\n",
      "After State:  The ingredients are blended together.\n"
     ]
    }
   ],
   "source": [
    "batch_id=0\n",
    "inst_id=2\n",
    "inst_prompt=all_prompts[inst_id][batch_id]\n",
    "inst_action=all_actions[inst_id][batch_id]\n",
    "im_prev=all_outs[inst_id][batch_id]\n",
    "im_next=all_outs[inst_id+1][batch_id]\n",
    "print(\"Action: \", inst_action)\n",
    "print(\"After State: \", inst_prompt)\n",
    "\n",
    "# Calculate the width and height of the new image\n",
    "new_width = im_prev.width + im_next.width\n",
    "new_height = max(im_prev.height, im_next.height)\n",
    "\n",
    "# Create a new image with white background\n",
    "new_image = Image.new(\"RGB\", (new_width, new_height), (255, 255, 255))\n",
    "\n",
    "# Paste the first image at the leftmost position\n",
    "new_image.paste(im_prev, (0, 0))\n",
    "\n",
    "# Paste the second image at the position next to the first image\n",
    "new_image.paste(im_next, (im_prev.width, 0))\n",
    "\n",
    "# Display or save the new image\n",
    "#new_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1][action_id]['description'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ali=pipe.text_encoder(torch.tensor(pipe.tokenizer(prompt)['input_ids']).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ali.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f0dba2db99f7dd5dec0bb31206f9203dfd4f0564fb3fd5500cd94a61f7e2b3b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('envPP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
